{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "id": "2FOIKGkcbUO1",
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "# ==========================\n",
    "# 0. IMPORTS\n",
    "# ==========================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Input\n",
    "\n",
    "import matplotlib.pyplot as plt\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# ==========================\n",
    "# 1. LOAD DATA\n",
    "# ==========================\n",
    "df = pd.read_csv(\"cleaned_hotels_all_months.csv\")\n",
    "\n",
    "print(\"Shape:\", df.shape)\n",
    "print(\"Columns:\", df.columns.tolist())\n",
    "df.head()\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 590
    },
    "id": "GqoiyXiuclTP",
    "outputId": "e28fe863-3585-4f2e-c644-a26bbd79aa93"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# ==========================\n",
    "# 2. DEFINE TARGETS (3 KPIs)\n",
    "# ==========================\n",
    "# Adjust these names if your columns differ slightly\n",
    "TARGET_NAMES = [\"Occupancy_Rate\", \"ADR_Month\", \"RevPAR_month\"]\n",
    "\n",
    "for t in TARGET_NAMES:\n",
    "    if t not in df.columns:\n",
    "        raise ValueError(f\"Target column '{t}' not found in dataframe. Check column names.\")\n",
    "\n",
    "y = df[TARGET_NAMES]\n",
    "\n",
    "# ==========================\n",
    "# 3. DEFINE FEATURES\n",
    "# ==========================\n",
    "# We'll drop:\n",
    "# - hotel identifier columns\n",
    "# - Month (we could one-hot encode later if desired)\n",
    "# - the target columns we are trying to predict\n",
    "drop_cols = [\"Hotel Name\", \"Month\"]  # adjust if your dataset uses different labels\n",
    "feature_cols = [c for c in df.columns if c not in drop_cols + TARGET_NAMES]\n",
    "\n",
    "print(\"Feature columns used:\")\n",
    "print(feature_cols)\n",
    "\n",
    "X = df[feature_cols]\n",
    "\n",
    "# Keep only numeric features (just in case there are any leftover objects)\n",
    "X = X.select_dtypes(include=[np.number])\n",
    "\n",
    "print(\"Final feature columns (numeric only):\")\n",
    "print(X.columns.tolist())\n",
    "print(\"X shape:\", X.shape, \"y shape:\", y.shape)\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J6VVr1wQcvUD",
    "outputId": "6e41061d-8654-42bd-f583-cbe7e601f999",
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# ==========================\n",
    "# 4. TRAIN–TEST SPLIT\n",
    "# ==========================\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(\"Train size:\", X_train.shape[0], \"Test size:\", X_test.shape[0])\n",
    "\n",
    "# ==========================\n",
    "# 5. FEATURE SCALING\n",
    "# ==========================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3lr2QweRczyi",
    "outputId": "dc252073-f28e-47c1-be39-5a2b58c63873"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# ==========================\n",
    "# 6. METRIC FUNCTIONS\n",
    "# ==========================\n",
    "def regression_metrics(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Compute R², RMSE, MAE, MAPE for multi-output regression.\n",
    "    Returns metrics averaged across all 3 targets.\n",
    "    \"\"\"\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "\n",
    "    r2_list, rmse_list, mae_list, mape_list = [], [], [], []\n",
    "\n",
    "    for i in range(y_true.shape[1]):\n",
    "        yt = y_true[:, i]\n",
    "        yp = y_pred[:, i]\n",
    "\n",
    "        r2 = r2_score(yt, yp)\n",
    "        rmse = np.sqrt(mean_squared_error(yt, yp))\n",
    "        mae = mean_absolute_error(yt, yp)\n",
    "        mape = np.mean(np.abs((yt - yp) / np.maximum(np.abs(yt), 1e-8))) * 100\n",
    "\n",
    "        r2_list.append(r2)\n",
    "        rmse_list.append(rmse)\n",
    "        mae_list.append(mae)\n",
    "        mape_list.append(mape)\n",
    "\n",
    "    metrics = {\n",
    "        \"R²_mean\": np.mean(r2_list),\n",
    "        \"RMSE_mean\": np.mean(rmse_list),\n",
    "        \"MAE_mean\": np.mean(mae_list),\n",
    "        \"MAPE_mean\": np.mean(mape_list),\n",
    "    }\n",
    "    return metrics\n"
   ],
   "metadata": {
    "id": "Y0iNgunOc2_s"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# ==========================\n",
    "# 7. NEURAL NETWORK MODEL (MLP)\n",
    "# ==========================\n",
    "\n",
    "# Create the Neural Network model\n",
    "mlp = MLPRegressor(\n",
    "    hidden_layer_sizes=(64, 64),  # two hidden layers with 64 neurons each\n",
    "    activation='relu',\n",
    "    solver='adam',\n",
    "    max_iter=2000,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"Training Neural Network (MLP)...\")\n",
    "mlp.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict on test set\n",
    "y_pred_mlp = mlp.predict(X_test_scaled)\n",
    "\n",
    "mlp_metrics = regression_metrics(y_test, y_pred_mlp)\n",
    "print(\"\\nNeural Network (MLP) Performance:\")\n",
    "for k, v in mlp_metrics.items():\n",
    "    print(f\"  {k}: {v:.4f}\")\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9COi7tDic5b4",
    "outputId": "e932f05b-0dcb-4e79-ae48-e8987ab203f9"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# ==========================\n",
    "# 8. SVM MODEL (SVR)\n",
    "# ==========================\n",
    "svr_base = SVR(kernel=\"rbf\", C=10, epsilon=0.1)\n",
    "svr = MultiOutputRegressor(svr_base)\n",
    "\n",
    "print(\"\\nTraining SVM (SVR with RBF kernel)...\")\n",
    "svr.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict on test set\n",
    "y_pred_svr = svr.predict(X_test_scaled)\n",
    "\n",
    "svr_metrics = regression_metrics(y_test, y_pred_svr)\n",
    "print(\"\\nSVM (SVR) Performance:\")\n",
    "for k, v in svr_metrics.items():\n",
    "    print(f\"  {k}: {v:.4f}\")\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oT6xUS6Vc82K",
    "outputId": "786a13a7-5ef8-4010-fecf-ef53a2931fc8"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# ==========================\n",
    "# 9. LSTM MODEL (Keras)\n",
    "# ==========================\n",
    "\n",
    "# For LSTM, Keras expects input shape: (samples, timesteps, features)\n",
    "# Here we treat each row as a 1-timestep sequence.\n",
    "X_train_lstm = X_train_scaled.reshape((X_train_scaled.shape[0], 1, X_train_scaled.shape[1]))\n",
    "X_test_lstm = X_test_scaled.reshape((X_test_scaled.shape[0], 1, X_test_scaled.shape[1]))\n",
    "\n",
    "n_features = X_train_scaled.shape[1]\n",
    "n_outputs = y_train.shape[1]\n",
    "\n",
    "# Build the LSTM model\n",
    "lstm_model = Sequential([\n",
    "    Input(shape=(1, n_features)),      # timesteps = 1, features = n_features\n",
    "    LSTM(64, activation='tanh'),       # 64 LSTM units\n",
    "    Dense(64, activation='relu'),      # dense layer\n",
    "    Dense(n_outputs)                   # output layer with 3 neurons (for 3 targets)\n",
    "])\n",
    "\n",
    "lstm_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mse'\n",
    ")\n",
    "\n",
    "lstm_model.summary()\n",
    "\n",
    "# Train the LSTM model\n",
    "print(\"\\nTraining LSTM model...\")\n",
    "history = lstm_model.fit(\n",
    "    X_train_lstm, y_train.values,\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    validation_split=0.2,\n",
    "    verbose=0  # set to 1 if you want to see training progress\n",
    ")\n",
    "\n",
    "# Predict on test set\n",
    "y_pred_lstm = lstm_model.predict(X_test_lstm)\n",
    "\n",
    "lstm_metrics = regression_metrics(y_test.values, y_pred_lstm)\n",
    "print(\"\\nLSTM Model Performance:\")\n",
    "for k, v in lstm_metrics.items():\n",
    "    print(f\"  {k}: {v:.4f}\")\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 381
    },
    "id": "1UeiOZaedFZv",
    "outputId": "6c7fa9de-6a83-4a72-811b-b17daf2f5687"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(history.history[\"loss\"], label=\"Train Loss\")\n",
    "plt.plot(history.history[\"val_loss\"], label=\"Val Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"MSE Loss\")\n",
    "plt.title(\"LSTM Training Curve\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 410
    },
    "id": "psvjkANXdJlN",
    "outputId": "6fe4c121-2dcb-4681-87ff-e8771b138b6a"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# ==========================\n",
    "# 10. COMPARE ALL THREE MODELS\n",
    "# ==========================\n",
    "results = pd.DataFrame([\n",
    "    [\"Neural Network (MLP)\", mlp_metrics[\"R²_mean\"], mlp_metrics[\"RMSE_mean\"],\n",
    "     mlp_metrics[\"MAE_mean\"], mlp_metrics[\"MAPE_mean\"]],\n",
    "    [\"SVM (SVR)\", svr_metrics[\"R²_mean\"], svr_metrics[\"RMSE_mean\"],\n",
    "     svr_metrics[\"MAE_mean\"], svr_metrics[\"MAPE_mean\"]],\n",
    "    [\"LSTM (Keras)\", lstm_metrics[\"R²_mean\"], lstm_metrics[\"RMSE_mean\"],\n",
    "     lstm_metrics[\"MAE_mean\"], lstm_metrics[\"MAPE_mean\"]],\n",
    "],\n",
    "    columns=[\"Model\", \"R²_mean\", \"RMSE_mean\", \"MAE_mean\", \"MAPE_mean\"]\n",
    ")\n",
    "\n",
    "display(results.round(4))\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "id": "BVRtIvTkdQub",
    "outputId": "b89f531c-d25c-4f1e-dadf-43fdd4f934e0"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# ==========================\n",
    "# 11. PREDICT ON FAKE HOTELS\n",
    "# ==========================\n",
    "\n",
    "# Create a few example hotels. Adjust ranges to be realistic.\n",
    "fake_hotels = pd.DataFrame([\n",
    "    {\n",
    "        \"Rating\": 4.2,\n",
    "        \"Room Rate\": 220,\n",
    "        \"Competitor density\": 3000,\n",
    "        \"Distance to Closest Station\": 0.3,\n",
    "        \"Property Type\": 4,\n",
    "        \"Avg High Season Rate\": 260,\n",
    "        \"Avg Low Season Rate\": 180,\n",
    "        # Add any other feature_cols that exist in your dataset,\n",
    "        # using reasonable values:\n",
    "        **{\n",
    "            col: X[col].mean() for col in X.columns\n",
    "            if col not in [\n",
    "                \"Rating\", \"Room Rate\", \"Competitor density\",\n",
    "                \"Distance to Closest Station\", \"Property Type\",\n",
    "                \"Avg High Season Rate\", \"Avg Low Season Rate\"\n",
    "            ]\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"Rating\": 4.8,\n",
    "        \"Room Rate\": 400,\n",
    "        \"Competitor density\": 6000,\n",
    "        \"Distance to Closest Station\": 0.1,\n",
    "        \"Property Type\": 5,\n",
    "        \"Avg High Season Rate\": 500,\n",
    "        \"Avg Low Season Rate\": 300,\n",
    "        **{\n",
    "            col: X[col].mean() for col in X.columns\n",
    "            if col not in [\n",
    "                \"Rating\", \"Room Rate\", \"Competitor density\",\n",
    "                \"Distance to Closest Station\", \"Property Type\",\n",
    "                \"Avg High Season Rate\", \"Avg Low Season Rate\"\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "])\n",
    "\n",
    "# Reorder columns to match training features exactly\n",
    "fake_hotels = fake_hotels[X.columns]\n",
    "\n",
    "# Scale\n",
    "fake_scaled = scaler.transform(fake_hotels)\n",
    "fake_lstm = fake_scaled.reshape((fake_scaled.shape[0], 1, fake_scaled.shape[1]))\n",
    "\n",
    "# Predictions\n",
    "fake_pred_mlp = mlp.predict(fake_scaled)\n",
    "fake_pred_svr = svr.predict(fake_scaled)\n",
    "fake_pred_lstm = lstm_model.predict(fake_lstm)\n",
    "\n",
    "for i in range(fake_hotels.shape[0]):\n",
    "    print(f\"\\n=== Fake Hotel #{i+1} ===\")\n",
    "    print(\"Input features:\")\n",
    "    display(fake_hotels.iloc[i:i+1])\n",
    "\n",
    "    print(\"Neural Network (MLP) predictions:\")\n",
    "    for j, t in enumerate(TARGET_NAMES):\n",
    "        print(f\"  {t}: {fake_pred_mlp[i][j]:.2f}\")\n",
    "\n",
    "    print(\"SVM (SVR) predictions:\")\n",
    "    for j, t in enumerate(TARGET_NAMES):\n",
    "        print(f\"  {t}: {fake_pred_svr[i][j]:.2f}\")\n",
    "\n",
    "    print(\"LSTM predictions:\")\n",
    "    for j, t in enumerate(TARGET_NAMES):\n",
    "        print(f\"  {t}: {fake_pred_lstm[i][j]:.2f}\")\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 717
    },
    "id": "cmsF-hQcdVr7",
    "outputId": "b476ca1f-edd8-46f2-ed52-474bd9f9d4cd"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Graphs for Neural Network"
   ],
   "metadata": {
    "id": "N7pE1C7KhNA4"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def plot_pred_vs_actual(y_true, y_pred, target_name):\n",
    "    plt.figure(figsize=(4,4))\n",
    "    plt.scatter(y_true, y_pred, alpha=0.6)\n",
    "    plt.plot([min(y_true), max(y_true)],\n",
    "             [min(y_true), max(y_true)],\n",
    "             'r--')\n",
    "    plt.xlabel(\"Actual\")\n",
    "    plt.ylabel(\"Predicted\")\n",
    "    plt.title(f\"Predicted vs Actual - {target_name}\")\n",
    "    plt.show()\n",
    "\n",
    "# Plot for NN (MLP)\n",
    "for i, target in enumerate(TARGET_NAMES):\n",
    "    plot_pred_vs_actual(y_test[target], y_pred_mlp[:, i], target)\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "MUHG6INXf3bM",
    "outputId": "2d1a60f6-1ef5-4a76-8b92-42ec6b1dc4f8"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "6LGmI55hhJyJ"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Across all three KPIs, the Neural Network (MLP) demonstrated the strongest overall performance, achieving:\n",
    "\n",
    "R² (mean): 0.36\n",
    "\n",
    "RMSE (mean): 28.78\n",
    "\n",
    "MAE (mean): 19.83\n",
    "\n",
    "MAPE (mean): 13.36%\n",
    "\n",
    "A MAPE of ~13% indicates that the model’s predictions are, on average, within ~87% accuracy. This is a strong baseline given the limited dataset, synthetic seasonal adjustments, and the variability inherent in hotel demand patterns.\n",
    "\n",
    "The SVM model achieved the highest R² overall (0.44) but suffered from significantly higher RMSE, suggesting inconsistent and less stable predictions.\n",
    "The LSTM model underperformed (R² ≈ −0.06), which is expected given that the data is not sequential time-series—the structure LSTMs are designed for."
   ],
   "metadata": {
    "id": "cfclVmPugcj3"
   }
  }
 ]
}
